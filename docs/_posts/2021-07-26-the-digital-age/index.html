<style>
    .sup-adjust sup{
        color:red;
        outline: 1px solid orange;
        display: inline-block;
        margin-left: -0.2em
    }
</style>
<h1>The Digital Age</h1>
<p>In the spring of 1942, as World War II was raging, the U.S. National Defense Research Committee convened a meeting of scientists and engineers to consider devices to aim and fire anti-aircraft guns. The <em>Blitzkrieg</em>, a brilliant military tactic based on rapid attacks by German dive bombers, made the matter urgent. The committee examined a number of designs, which they noticed fell into two broad categories. One directed antiaircraft fire by constructing a mechanical or electrical analog of the mathematical equations of fire control, for example, by machining a camshaft whose profile followed an equation of motion. The other solved the equations numerically—as with an ordinary calculating machine, only with fast electrical pulses instead of mechanical counters. One member of the committee, Bell Telephone Laboratories mathematician George Stibitz, felt that the term <em>pulse</em> was not quite right. He suggested another term that he felt was more descriptive: <em>digital</em>. The word referred to the method of counting on one’s fingers, or digits. It has since become the adjective that defines social, economic, and political life in the twenty-first century<span class="sup-adjust">.<sup>1</sup></span></p>
<p>It took more than just the coining of a term to create the digital age, but that age does have its origins in secret projects initiated or conducted during World War II. Most histories of computing, which purport to cover the full range of the topic, do not explain how such an invention, intended as a high-speed replacement for calculators during the war, could have led to such a far-reaching social impact.</p>
<p>Nor do those wartime developments, as significant as they were, explain the adoption of digital techniques for communications. That took place not during World War II but two decades later, when an agency of the U.S. Defense Department initiated a program to interconnect defense computers across the United States. That combination of computing and communications unleashed a flood of social change, in the midst of which we currently live.</p>
<p>Telecommunications, like computing, has a long and well-documented history, beginning with the Morse and Wheatstone electric telegraphs of the mid-nineteenth century, followed by the invention and spread of the telephone by Alexander Graham Bell, Elisha Gray, Thomas Edison, and others later that century. What was different about the 1960s computer networks? Nearly every social and business phenomenon we associate with the Internet was anticipated by similar uses of the telegraph a century earlier<span class="sup-adjust">.<sup>2</sup></span> The telegraph, combined with the undersea cable, did transform society, yet the transformation effected by the more recent application of a digital paradigm seems to be many times greater.</p>
<p>It is dangerous to apply modern terms to events of the past, but one may violate this rule briefly to note that the electric telegraph, as refined by Samuel Morse in the 1840s, was a proto “digital”device. It used pulses, not continuous current, and it employed a code that allowed it to send messages rapidly and accurately over long distances with a minimum number of wires or apparatus. Typesetters had long known that certain letters (e.g., e, t, a) were used more frequently than others, and on that basis the codes chosen for those letters were shorter than the others. A century later mathematicians placed this ad hoc understanding of telecommunications on a theoretical basis. What came to be called information theory emerged at the same time as, and independent of, the first digital computers in the 1930s and 1940s. Binary (base-2) arithmetic, bits, and bytes (8-bit coded characters) are familiar at least in name to modern users of digital devices. The roots of that theory are less well known, but those roots made the modern digital age possible.</p>
<p>Many histories of computing begin with Charles Babbage, the Englishman who tried, and failed, to build an “Analytical Engine”in the mid-nineteenth century—the same time as Morse was developing the telegraph<span class="sup-adjust">.<sup>3</sup></span> The reason is that Babbage’s design—what we now call the architecture of his machine—was remarkably modern. It embodied the basic elements that were finally realized in the computers built during World War II. We now see, however, that to begin with Babbage is to make certain assumptions. What exactly is a “computer”? And what is its relation to the digital age that we are living in today?</p>
<h2>The Components of Computing</h2>
<p>Computing represents a convergence of operations that had been mechanized to varying degrees in the past. Mechanical aids to calculation are found in antiquity, when cultures developed aids to counting and figuring such as pebbles (Latin <em>calculi</em>, from which comes the term <em>calculate</em>), counting boards (from which comes the modern term <em>countertop</em>), and the abacus—all of which survive into this century. Although it may seem arbitrary, the true mechanization of calculation began when inventors devised ways not only to record numbers but to add them, in particular to automatically carry a digit form one column to the next when necessary, especially for carries like 999 + 1. That began with Pascal’s adding machine of 1642, or with a device invented by Wilhelm Schickard in 1623. Leibniz extended Pascal’s invention by developing a machine, a few decades later, that could multiply as well as add. The mechanisms by which these devices operated lay dormant until the nineteenth century, when advancing commerce and business created a demand that commercial manufacturers sought to fill. Toward the end of that century, mechanical calculators of intricate design appeared in Europe and in the United States. The Felt Comptometer, invented in the 1880s, was one of the first successful calculators, owing to its simplicity, speed, and reliable operation. The Burroughs adding machine, invented by William S. Burroughs around the same time, also was a commercial success. Burroughs survived as a supplier of electronic computers into the 1980s and is the ancestor of today’s Unisys. In Europe, machines supplied by companies including Brunsviga and Odhner were also widely sold. On these machines, the user set numbers on a set of wheels rather than press keys, but they worked on similar principles.</p>
<p>As significant as calculation were two additional functions: the automatic storage and retrieval of information in coded form and the automatic execution of a sequence of operations. That is the reason historians began with the Analytical Engine that Charles Babbage attempted to build in the nineteenth century. Babbage never completed that machine, for reasons that only in part had to do with the state of mechanical engineering at the time. In the 1830s, when Babbage was sketching out ideas for such an engine, neither he nor anyone else could draw on electrical technology to implement his ideas; everything had to be done mechanically. Given the necessary level of complexity that a computer must have, a mechanical computer of decent power was not practical then, and not today either. The recent successful reconstruction, at great expense, of Babbage’s other project, the Difference Engine, proves this point<span class="sup-adjust">.<sup>4</sup></span> It works, but the Difference Engine’s capabilities are nowhere near those of the Analytical Engine. An analogy might be to compare Babbage’s vision with Leonardo’s sketches for a flying machine: Leonardo’s vision was sound, but heavier-than-air flight had to await the invention of the gasoline engine to provide sufficient power in a lightweight package.</p>
<p>By this measure, one might begin the history of computing in the late nineteenth century, when the American inventor Herman Hollerith developed, for the 1890 U.S. Census, a method of storing information coded as holes punched into cards. Hollerith developed not just the punched card but a suite of machines that used cards to sort, retrieve, count, and perform simple calculations on data punched onto cards. The devices he developed combined complex mechanisms with electromagnets and motors to perform operations. The use of electricity was not required. A rival inventor, James Powers, produced pure mechanical punched card equipment to avoid infringing on Hollerith’s parents, but in practice the flexibility that Hollerith’s use of electricity gave his machines was an advantage as the machinery was called on to perform ever more complex operations. By the time of World War II, electrical circuits took on an even greater significance, not just as a carrier of information but also as a way of performing computing operations at high speeds—a property that in theory is not required of a true computer but in practice is paramount.</p>
<p>The inherent flexibility of Hollerith’s system of machines built around the punched card led to many applications beyond that of the U.S. Census. Hollerith founded the Tabulating Machine Company to market his inventions; it was later combined with other companies to form the Computing-Tabulating-Recording Company (C-T-R), and in 1924, the new head of C-T-R, Thomas Watson, changed the name to the International Business Machines Corporation, today’s IBM. In 1927 the Remington Rand Corporation acquired the rival Powers Accounting Machine Company, and these two would dominate business accounting for the next four decades.</p>
<p>It is not known where Hollerith got the idea of storing information in the form of holes punched onto card stock, but the concept was not original with him. Babbage proposed using punched cards to control his Analytical Engine, an idea he borrowed from the looms invented by the Frenchman Joseph-Marie Jacquard (1752–1834), who in the nineteenth century used punched cards to control the weaving of cloth by selectively lifting threads according to a predetermined pattern (Jacquard cloth is still woven to this day). Jacquard looms were common in Hollerith’s day, so he was probably familiar with the way punched cards controlled them. However, there is a crucial difference between Jacquard’s and Hollerith’s systems: Jacquard used cards for control, whereas Hollerith used them for storage of data. Eventually IBM’s punched card installations would also use the cards for control. It is fundamental to the digital paradigm that information stored in digital form can be used for storage, control, or calculation, but an understanding of that would not come until decades later. Before World War II, the control function of a punched card installation was carried out by people: they carried decks of cards from one device to another, setting switches or plugging wires on the devices to perform specific calculations, and then collecting the results.</p>
<p>The concept of automatic control, the ancestor of what we now call software, is a third component of computing, and it too has a history that can be traced back to antiquity. Jacquard’s invention was an inside-out version of a device that had been used to control machinery for centuries: a cylinder on which were mounted pegs, which tripped levers as it rotated. These had been used in medieval clocks that executed complex movements at the sounding of each hour; they are also found in wind-up toys, including music boxes. Babbage’s Analytical Engine was to contain a number of such cylinders to carry more detailed sequences of operations as directed by the punched cards; today we might call this the computer’s microprogramming, or read-only memory (ROM). Continuous control of many machines, including classic automobile engines, is effected by cams, which direct the movement of other parts of the machine in a precisely determined way. Unlike cylinders or camshafts, punched cards can be stacked in an arbitrarily long sequence. It is also easy to substitute a short sequence of cards in the stack to tailor the machine for a specific problem, but Jacquard looms used cards that were tied to one another, making any modification to the control difficult.</p>
<p>Control, storage, calculation, the use of electrical or electronic circuits: these attributes, when combined, make a computer. To them we add one more: communication—the transfer of coded information by electrical or electronic means across geographical distances. This fifth attribute was lacking in the early electronic computers built in the 1930s and 1940s. It was the Defense Department’s Advanced Research Projects Agency (ARPA)’s mission, beginning in the 1960s, to reorient the digital computer to be a device that was inherently networked, for which communication was as important to it as calculation, storage, or control.</p>
<p>The origins of the electric telegraph and telephone are well known, but their relationship to computing is complex. In 1876, Alexander Graham Bell publicly demonstrated a telephone: a device that transmitted the human voice over wires. The telephone's relationship to the invention of the computer was indirect. Computers today operate by electrical circuits that allow only one of two states: in modem terms they are both digital, as described above, and binary: they count in base 2. The telephone operated by inducing a continuous variation of current based on the variations of the sound of a person's voice: in today's terms, it was an analog device. Like <em>digital</em>, that term was also unknown before the late 1930s, and therefore not entirely proper to use here. Devices that compute by analogy were once common. The slide rule, for example, was in common use into the 1970s, when it was driven out by the pocket calculator. During the first decades of electronic digital computing, the 1940s and 1950s, there were debates over the two approaches, with analog devices fading into obscurity. Nature is in a fundamental sense continuous, as in the infinite variations of the human voice or the sounds of a musical instrument. But the digital paradigm has prevailed, even in telephony. During World War II, Bell Telephone Laboratories developed a machine that translated voice signals into discrete pulses, encoded them, and reconstituted the voice at the other end-this was to enable Franklin D. Roosevelt and Winston Churchill to speak to each other securely<span class="sup-adjust">.<sup>5</sup></span> That was a one-time development, although eventually all phone calls were encoded this way, in what is now called pulse code modulation. The technique is used not so much for secrecy (although that can be done when necessary), but to exploit of the inherent advantages of digital electronics.</p>
<p>Bell's successful defense of his patent, and the subsequent establishment of a wealthy, regulated monopoly to provide telephone service in the United States, led to generous funding for the Bell Telephone Laboratories, which conducted fundamental research in the transmission of information, broadly defined. The role of one Bell Labs mathematician, George Stibitz, has already been mentioned. It was a team of Bell Labs researchers who invented the transistor in the 1940s; two decades later, another Bell Labs team developed the Unix operating system, to mention only the most visible fruits of Bell Laboratories' research. And it was at Bell Labs where much of the theory of information coding, transmission, and storage was developed.</p>
<p>Once again: the modern computer is a convergence of separate streams of information handling, each with its own rich tradition of technological history. Each of the streams described thus far played a major role. One could add other antecedents such as the development of radio, motion pictures, and photography. The line of mechanical calculators seems to be at the forefront, yet it was the Hollerith system of punched cards, centered around a machine called the tabulator, that had a greater influence. Besides the tabulator, two other critical devices were employed: a key punch, by which a human operator keyed in data, and a sorter, which sorted cards based on the presence or absence of a hole in a desired column. In the early decades of the twentieth century, other devices were added to a typical installation, but these were the main ones.</p>
<h2>From Tabulator to Computer, 1890–1945</h2>
<p>The tabulator kept a record of how many cards had a hole punched in each of its columns. The early tabulators recorded the numbers on a dial that resembled a clock face; later a more familiar counter was used. Although it is hard to imagine that such a basic function could be so important, it was not until the 1920s that other arithmetic functions were provided. What made the tabulator so important was the flexible way it could be used, based on the information punched into different columns, and the way its use could be combined with the other devices, especially the sorter, to perform what we would now call sophisticated data processing. Information, once it was punched onto a card, could be used and reused in many ways. That roomful of equipment was what the early electronic computers replicated; its &quot;program&quot; was carried out by human beings carrying decks of cards from one machine to another and changing the settings on the various machines as the cards were run through them.</p>
<p>Communications, within and outside that room, was also present, in an ad hoc fashion. Human beings carried data from one machine to another as decks of cards. The electric telegraph carried information to and from the installation. Among the first, outside the government, to adopt punched card accounting in the United States were railroads. And railroads were early adopters of the telegraph as well, because they were the first significant business whose managers had to coordinate operations over wide geographical areas. The railroad rights of way became a natural corridor for the erection of wires across the continent, to an extent that people assume the two technologies could not have existed without each other. That is an exaggeration but not far from the truth. Although times have changed, modern overland Internet traffic is carried on fiber-optic lines, which are often laid underground (not on poles) along railroad rights of way.</p>
<p>If the telegraph apparatus did not physically have a presence in the punched card installation, its information was incorporated into the data processed in that room. Railroad operators were proficient at using the Morse code and were proud of their ability to send and receive the dots and dashes accurately and quickly. Their counterparts in early commercial and military aviation did the same, using the &quot;wireless&quot; telegraph, as radio was called. What worked for railroads and aircraft was less satisfactory for other businesses, however. Among the many inventions credited to Thomas Edison was a device that printed stock market data sent by telegraph on a ''ticker tape,&quot; so named because of the sound it made. The physical ticker tape has been replaced by electronic displays, but the terse symbols for the stocks and the &quot;crawling&quot; data have been carried over into the electronic era<span class="sup-adjust">.<sup>6</sup></span> Around 1914, Edward E. Kleinschmidt, a German immigrant to the United States, developed a series of machines that combined the keyboard and printing capabilities of a typewriter with the ability to transmit messages over wires<span class="sup-adjust">.<sup>7</sup></span> In 1928 the company he founded changed its name to the Teletype Corporation, which AT&amp;T purchased two years later. AT&amp;T, the telephone monopoly, now became a supplier of equipment that transmitted text as well as voice (see figure 1.1).</p>
<figure>
<p><img src="/assets/bell_labs.jpg" alt="(a) H. L. Marvin operating a Bell Labs specialized calculator using a modified Teletype, 1940. (b) Control panel for a Bell Labs computer used for fire control, circa 1950."></p>
<figcaption>
Figure 1.1 Computing at Bell Laboratories designed by George Stibitz and using modified telephone switching equipment. (a) H. L. Marvin operating a Bell Labs specialized calculator using a modified Teletype, 1940. (b) Control panel for a Bell Labs computer used for fire control, circa 1950. (Source: Lucent/ Alcatel Bell Laboratories)
</figcaption>
</figure>
<p>The Teletype (the name referred to the machine as well as to the company that manufactured it) was primitive by today's standards: slow, noisy, and with few symbols other than the uppercase letters of the alphabet and the digits <code>0</code> through <code>9</code>. But it found a large market, providing the communications component to the information processing ensemble described above. The machine took its place alongside other information handling equipment in offices, the government, and the military. It also entered our culture. Radio newscasters liked to have a Teletype chattering in the background as they read the news, implying that what they were reading was &quot;ripped from the wires.&quot; Jack Kerouac did not type the manuscript for his Beat novel, <em>On the Road</em>, on a continuous reel of Teletype paper, but that is the legend. In the 1970s manufacturers of small computers modified the Teletype to provide an inexpensive terminal for their equipment. Bill Gates and Paul Allen, the founders of Microsoft, marketed their first software products on rolls of teletype tape. Among those few extra symbols on a Teletype keyboard was the @ sign, which in 1972 was adopted as the marker dividing a person's e-mail address from the computer system the person was using. Thus, we owe the symbol of the Internet age to the Teletype (see figure 1.2).</p>
<h2>The Advent of Electronic Computing</h2>
<p>In the worlds of U.S. commerce and government, these systems reached a pinnacle of sophistication in the 1930s, ironically at a time of economic depression. Besides IBM and Remington Rand supplying punched card equipment, companies like National Cash Register (later NCR), Burroughs, Victor Adding Machine (later Victor Comptometer), and others supplied mechanical equipment: adding machines, cash registers, accounting machines, time clocks, &quot;computing&quot; scales (which priced items by weight), duplicating machines, and electric typewriters. Supplementing these were systems that used little mechanization but were crucial to controlling the flow of information: card filing systems, standardized forms in multiple copies with carbon paper, bound or loose-leaf ledgers, and more<span class="sup-adjust">.<sup>8</sup></span> The digital computer upended this world, but it did not happen overnight. Many of the firms noted here entered the commercial computer industry, with IBM, Remington Rand, and Burroughs among the top U.S. computer manufacturers through the 1960s<span class="sup-adjust">.<sup>9</sup></span></p>
<figure>
<p><img src="/assets/teletype.jpg" alt="The Teletype ASR-33. The Teletype had only uppercase letters, numbers, and a few special characters."></p>
<p><img src="/assets/punch_roll.jpg" alt="Teletypes were used as the main input-output device for the early personal computers, until inexpensive video terminals became available. This piece of Teletype tape contains an interpreter for the BASIC programming language, Microsoft's first product"></p>
<figcaption>
Figure 1.2 (left) The Teletype ASR-33. The Teletype had only uppercase letters, numbers, and a few special characters. In 1972 Ray Tomlinson, an engineer at Bolt Beranek and Newman in Cambridge, Massachusetts, chose the @ sign (shift-p) to separate the recipient of an e-mail message from the host machine to which that message was addressed; it has since become the symbol of the Internet age. (Source: Digital Equipment Corporation, now Hewlett-Packard) (above) Teletypes were used as the main input-output device for the early personal computers, until inexpensive video terminals became available. This piece of Teletype tape contains an interpreter for the BASIC programming language, Microsoft's first product. (Credit: Smithsonian Institution)
</figcaption>
</figure>
<p>As is so often the case in the history of technology, at the precise moment when this system was functioning at its peak of efficiency, digital computing emerged to replace it. The most intense period of innovation was during World War II, but as early as the mid-1930s, the first indications of a shift were apparent. In hindsight, the reasons were clear. The systems in place by the 1930s were out of balance. The versatility of the punched card, which stored data that could be used and reused in a variety of ways, required that human beings first of all make a plan for the work that was to be done and then carry out that plan by operating the machines at a detailed level. Human beings had to serve as the interface with the adding machines, accounting machines, and other equipment, and with the nonmechanized patterns of paper flow that corporations and government agencies had established. For example, an eighty-column card, in spite of its great flexibility, was ill suited to store or print a person's mailing address. Thus, a company that periodically sent out bills required another machine, such as the American Addressograph: a metal plate on which the person's name and address were embossed, from which mailing labels were printed. A human being had to coordinate the operation of these two technologies<span class="sup-adjust">.<sup>10</sup></span></p>
<p>For many problems, especially those in science or engineering, a person operating a simple Comptometer or calculator could perform arithmetic quite rapidly, but she (and such persons typically were women) would be asked to carry out one sequence if interim results were positive, a different one if negative. The plan for her work would be specified in detail in advance and given to her on paper. Punched card equipment also had stops built into their operation, signaling to the operator to remove a deck of cards and proceed in a different direction depending on the state of the machine. The human beings who worked in some of these places-for example, astronomical observatories where data from telescope observations were reduced-had the job title &quot;computer&quot;: a definition that was listed as late as the 1970 edition of <em>Webster's New World Dictionary</em>. From the human computers, who alone had the ability to direct a sequence of operations as they worked, the term <em>computer</em> came to describe a machine.</p>
<p>A second reason that the systems of the 1930s fell short emerged during World War II: a need for high speed. The systems of the 1930s used electricity to route signals and transfer information from one part of a device to another and to communicate over long distances. Communication over the telegraph proceeded at high speed, but calculations were done at mechanical speeds, limited by Newton's laws that relate speed to the amount of energy required to move a piece of metal. As the complexity of problems increased, the need for high-speed operation came to the fore, especially in wartime applications like breaking an enemy code or computing the trajectory of a shell. High speeds could be achieved only by modifying another early twentieth-century invention, the vacuum tube, which had been developed for radio and telephone applications. Those modifications would not be easy, and substituting tubes for mechanical parts introduced a host of new problems, but with electronics, the calculations would not be bound not by Newton's laws, thus allowing calculating as well as computing to approach the speed of light.</p>
<p>These limiting factors, of automatic control and of electronic speeds, were &quot;reverse salients,&quot; in Thomas Hughes's term: impediments that prevented the smooth advance of information handling on a broad front (the term comes from World War I military strategy)<span class="sup-adjust">.<sup>11</sup></span> Solutions emerged simultaneously in several places beginning around 1936. The work continued during World War II at a faster pace, although under a curtain of secrecy, which in some respects mitigated the advantages of having funds and human resources made available to the computer's inventors.</p>
